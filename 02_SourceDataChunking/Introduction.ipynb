{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1) Chunking\n",
    "Chunking in Large Language Models (LLMs) refers to the process of dividing input data or text into multiple smaller, manageable pieces, ensuring that each chunk\n",
    "- retains its meaningful context and coherence without losing any critical information and\n",
    "- adheres to the model's token limit to preserve full context within the manageable processing window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a RAG pipeline, the goal is to provide the LLM with precisely the information needed for the specific task, and nothing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most crucial steps for enhancing the efficiency of LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“What should be the right chunking strategy in my solution” is one of the initial and fundamental decision a LLM practitioner must make while building advance RAG solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the world of multi-modal, splitting also applies to images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the video(link shared below), Greg Kamradt provides overview of different chunking strategies. These strategies can be leveraged as starting points to develop RAG based LLM application. They have been classified into five levels based on the complexity and effectiveness.\n",
    "\n",
    "https://www.youtube.com/watch?v=8OJC21T2SL4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot pass unlimited data to our language model. Reasons are:\n",
    "1. Context Length/ Token Limit: \n",
    "    - Limit on the amount of words/ tokens that a language model allows as an input.\n",
    "2. Signal to Noise: \n",
    "    - Language models perform better when you increase the signal to noise ratio.\n",
    "    - Distrecting information in the model's context window does tend to measurably destroy the performance of the overall application.  \n",
    "\n",
    "This approach significantly reduces the time and computational resources required for the LLM to process large amounts of data, as it only needs to interact with the relevant chunks instead of the entire documentation.  \n",
    "\n",
    "It also allows for real-time updates to the database. As product documentation evolves, the corresponding chunks in the vector database can be easily updated. This ensures that the chatbot always provides the most up-to-date information.  \n",
    "\n",
    "Finally, by focusing on semantically relevant chunks, the LLM can provide more precise and contextually appropriate responses, leading to improved customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************\n",
    "*************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Levels Of Text Splitting\n",
    "* **Level 1: [Character Splitting](#CharacterSplitting)** - Simple static character chunks of data\n",
    "* **Level 2: [Recursive Character Text Splitting](#RecursiveCharacterSplitting)** - Recursive chunking based on a list of separators\n",
    "* **Level 3: [Document Specific Splitting](#DocumentSpecific)** - Various chunking methods for different document types (PDF, Python, Markdown)\n",
    "* **Level 4: [Semantic Splitting](#SemanticChunking)** - Embedding walk based chunking\n",
    "* **Level 5: [Agentic Splitting](#AgenticChunking)** - Experimental method of splitting text with an agent-like system. Good for if you believe that token cost will trend to $0.00\n",
    "* **\\*Bonus Level:\\*** **[Alternative Representation Chunking + Indexing](#BonusLevel)** - Derivative representations of your raw text that will aid in retrieval and indexing\n",
    "\n",
    "**Notebook resources:**\n",
    "* [Video Overview]() - Walkthrough of this code with commentary\n",
    "* [ChunkViz.com](https://www.chunkviz.com/) - Visual representation of chunk splitting methods\n",
    "* [RAGAS](https://github.com/explodinggradients/ragas) - Retrieval evaluation framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import fitz # fitz is the legacy name for PyMuPDF library\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to build source file path\n",
    "def get_source_file(file_name: str) -> Path:\n",
    "    file_path = Path.cwd().parent.joinpath(\"99_Datasets\").joinpath(file_name)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(\"File not found\")\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to cleanse the input text\n",
    "import re\n",
    "def cleanse_text(text: str) -> str:\n",
    "    pattern = r\"\\s?RAG\\n|\\d+\\n\"\n",
    "    final_text = re.sub(pattern, \"\", text)\n",
    "    cleaned_text = final_text.replace(\"\\n\", \" \")\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to read pdf\n",
    "def read_pdf(pdf_path: Path) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "\n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "        pages.append(text)\n",
    "\n",
    "    combined_text = \" \".join(pages)\n",
    "    final_text = cleanse_text(combined_text)\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Retrieval-Augmented Generation? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of  a large language model, so it references an authoritative knowledge base outside  of its training data sources before generating a response. Large Language Models  (LLMs) are trained on vast volumes of data and use billions of parameters to  generate original output for tasks like answering questions, translating languages,  and completing sentences. RAG extends the already powerful capabilities of LLMs  to specific domains or an organization's internal knowledge base, all without the  need to retrain the model. It is a cost-effective approach to improving LLM output  so it remains relevant, accurate, and useful in various contexts. Why is Retrieval-Augmented Generation important? LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots  and other natural language processing (NLP) applications. The goal is to create  bots that can answer user questions in various contexts by cross-referencing  authoritative knowledge sources. Unfortunately, the nature of LLM technology  introduces unpredictability in LLM responses. Additionally, LLM training data is  static and introduces a cut-off date on the knowledge it has. Known challenges of LLMs include: Presenting false information when it does not have the answer. Presenting out-of-date or generic information when the user expects a  specific, current response. Creating a response from non-authoritative sources. Creating inaccurate responses due to terminology confusion, wherein different  training sources use the same terminology to talk about different things. You can think of the Large Language Model as an over-enthusiastic new employee  who refuses to stay informed with current events but will always answer every  question with absolute confidence. Unfortunately, such an attitude can negatively  impact user trust and is not something you want your chatbots to emulate! RAG is one approach to solving some of these challenges. It redirects the LLM to  retrieve relevant information from authoritative, pre-determined knowledge  sources. Organizations have greater control over the generated text output, and  users gain insights into how the LLM generates the response. What are the benefits of Retrieval-Augmented  Generation? RAG technology brings several benefits to an organization's generative AI efforts. Cost-effective implementation Chatbot development typically begins using a foundation model. Foundation  models (FMs) are API-accessible LLMs trained on a broad spectrum of  generalized and unlabeled data. The computational and financial costs of  retraining FMs for organization or domain-specific information are high. RAG is a  more cost-effective approach to introducing new data to the LLM. It makes  generative artificial intelligence (generative AI) technology more broadly  accessible and usable. Current information Even if the original training data sources for an LLM are suitable for your needs, it  is challenging to maintain relevancy. RAG allows developers to provide the latest  research, statistics, or news to the generative models. They can use RAG to  connect the LLM directly to live social media feeds, news sites, or other  frequently-updated information sources. The LLM can then provide the latest  information to the users. Enhanced user trust RAG allows the LLM to present accurate information with source attribution. The  output can include citations or references to sources. Users can also look up  source documents themselves if they require further clarification or more detail.  This can increase trust and confidence in your generative AI solution. More developer control With RAG, developers can test and improve their chat applications more  efficiently. They can control and change the LLM's information sources to adapt to  changing requirements or cross-functional usage. Developers can also restrict  sensitive information retrieval to different authorization levels and ensure the LLM  generates appropriate responses. In addition, they can also troubleshoot and  make fixes if the LLM references incorrect information sources for specific  questions. Organizations can implement generative AI technology more  confidently for a broader range of applications. How does Retrieval-Augmented Generation work? Without RAG, the LLM takes the user input and creates a response based on  information it was trained on—or what it already knows. With RAG, an information  retrieval component is introduced that utilizes the user input to first pull  information from a new data source. The user query and the relevant information  are both given to the LLM. The LLM uses the new knowledge and its training data  to create better responses. The following sections provide an overview of the  process. Create external data The new data outside of the LLM's original training data set is called external  data. It can come from multiple data sources, such as a APIs, databases, or  document repositories. The data may exist in various formats like files, database  records, or long-form text. Another AI technique, called embedding language  models, converts data into numerical representations and stores it in a vector  database. This process creates a knowledge library that the generative AI models  can understand. Retrieve relevant information The next step is to perform a relevancy search. The user query is converted to a  vector representation and matched with the vector databases. For example,  consider a smart chatbot that can answer human resource questions for an  organization. If an employee searches, \"How much annual leave do I have?\" the  system will retrieve annual leave policy documents alongside the individual  employee's past leave record. These specific documents will be returned because  they are highly-relevant to what the employee has input. The relevancy was  calculated and established using mathematical vector calculations and  representations. Augment the LLM prompt Next, the RAG model augments the user input (or prompts) by adding the relevant  retrieved data in context. This step uses prompt engineering techniques to  communicate effectively with the LLM. The augmented prompt allows the large  language models to generate an accurate answer to user queries. Update external data The next question may be—what if the external data becomes stale? To maintain  current information for retrieval, asynchronously update the documents and  update embedding representation of the documents. You can do this through  automated real-time processes or periodic batch processing. This is a common  challenge in data analytics—different data-science approaches to change  management can be used. The following diagram shows the conceptual flow of using RAG with LLMs. What is the difference between Retrieval-Augmented  Generation and semantic search? Semantic search enhances RAG results for organizations wanting to add vast  external knowledge sources to their LLM applications. Modern enterprises store  vast amounts of information like manuals, FAQs, research reports, customer  service guides, and human resource document repositories across various  systems. Context retrieval is challenging at scale and consequently lowers  generative output quality. Semantic search technologies can scan large databases of disparate information  and retrieve data more accurately. For example, they can answer questions such  as, \"How much was spent on machinery repairs last year?” by mapping the  question to the relevant documents and returning specific text instead of search  results. Developers can then use that answer to provide more context to the LLM. Conventional or keyword search solutions in RAG produce limited results for  knowledge-intensive tasks. Developers must also deal with word embeddings,  document chunking, and other complexities as they manually prepare their data.  In contrast, semantic search technologies do all the work of knowledge base  preparation so developers don't have to. They also generate semantically relevant  passages and token words ordered by relevance to maximize the quality of the  RAG payload. How can AWS support your Retrieval-Augmented  Generation requirements? Amazon Bedrock is a fully-managed service that offers a choice of high- performing foundation models—along with a broad set of capabilities—to build  generative AI applications while simplifying development and maintaining privacy  and security. With knowledge bases for Amazon Bedrock, you can connect FMs to  your data sources for RAG in just a few clicks. Vector conversions, retrievals, and  improved output generation are all handled automatically. For organizations managing their own RAG, Amazon Kendra is a highly-accurate  enterprise search service powered by machine learning. It provides an optimized  Kendra Retrieve API that you can use with Amazon Kendra’s high-accuracy  semantic ranker as an enterprise retriever for your RAG workflows. For example,  with the Retrieve API, you can: Retrieve up to 100 semantically-relevant passages of up to 200 token words  each, ordered by relevance. Use pre-built connectors to popular data technologies like Amazon Simple  Storage Service, SharePoint, Confluence, and other websites. Support a wide range of document formats such as HTML, Word, PowerPoint,  PDF, Excel, and text files. Filter responses based on those documents that the end-user permissions  allow. Amazon also offers options for organizations who want to build more custom  generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs,  built-in algorithms, and prebuilt ML solutions that you can deploy with just a few  clicks. You can speed up RAG implementation by referring to existing SageMaker  notebooks and code examples. Get started with Retrieval-Augmented Generation on AWS by creating a free  account today \n"
     ]
    }
   ],
   "source": [
    "source_file = get_source_file('rag.pdf')\n",
    "final_text = read_pdf(source_file)\n",
    "\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************\n",
    "*************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1) Level 1: Character Splitting / Fixed Size Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form.  \n",
    "\n",
    "This is the most crude and simplest method of segmenting the text. It breaks down the text into chunks of a specified number of characters, regardless of their content or structure.\n",
    "\n",
    "This method isn't recommended for any applications - but it's a great starting point for us to understand the basics.\n",
    "\n",
    "- **Pros:** Easy & Simple\n",
    "- **Cons:** Very rigid and doesn't take into account the structure and context of your text\n",
    "\n",
    "Concepts to know:\n",
    "\n",
    "- **Chunk Size** - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
    "- **Chunk Overlap** - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks.\n",
    "- **separator:** character(s) on which the text would be split on (default “”)\n",
    "\n",
    "Langchain and llamaindex framework offer `CharacterTextSplitter` and `SentenceSplitter` (default to spliting on sentences) classes for this chunking technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: get some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Retrieval-Augmented Generation (RAG) is the process of optimizing the output of  a large language model, so it references an authoritative knowledge base outside  of its training data sources before generating a response.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Implement character splitter using plain python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Retrieval-Augmented Gener',\n",
       " 'ation (RAG) is the proces',\n",
       " 's of optimizing the outpu',\n",
       " 't of  a large language mo',\n",
       " 'del, so it references an ',\n",
       " 'authoritative knowledge b',\n",
       " 'ase outside  of its train',\n",
       " 'ing data sources before g',\n",
       " 'enerating a response.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list that will hold the chunks\n",
    "chunks = []\n",
    "chunk_size = 25\n",
    "len_of_text = len(text)\n",
    "for i in range(0, len_of_text, chunk_size):\n",
    "    chunk = text[i:i+chunk_size]\n",
    "    chunks.append(chunk)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the result above, words are not broken down in a meaningful way.  \n",
    "Generation is splitted into Gener and ation. We are loosing the meaning of the word itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************\n",
    "*************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3: Implement character splitting using LangChain `CharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=25, chunk_overlap=0, separator='', strip_whitespace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `create_documents` method from `CharacterTextSplitter` to split our text.  \n",
    "\n",
    "Note: `create_documents` expects a list of texts, so if you just have a string (like we do) you'll need to wrap it in `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Retrieval-Augmented Gener', metadata={}),\n",
       " Document(page_content='ation (RAG) is the proces', metadata={}),\n",
       " Document(page_content='s of optimizing the outpu', metadata={}),\n",
       " Document(page_content='t of  a large language mo', metadata={}),\n",
       " Document(page_content='del, so it references an', metadata={}),\n",
       " Document(page_content='authoritative knowledge b', metadata={}),\n",
       " Document(page_content='ase outside  of its train', metadata={}),\n",
       " Document(page_content='ing data sources before g', metadata={}),\n",
       " Document(page_content='enerating a response.', metadata={})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks = text_splitter.create_documents([text])\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this time we have the same chunks, but they are in documents. These will play nicely with the rest of the LangChain world. Also notice how the trailing whitespace on the end of the 5th chunk is missing. This is because LangChain removes it, see [this line](https://github.com/langchain-ai/langchain/blob/f36ef0739dbb548cabdb4453e6819fc3d826414f/libs/langchain/langchain/text_splitter.py#L167) for where they do it. You can avoid this with `strip_whitespace=False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.1: Try using Chunk Overlap & Separators\n",
    "\n",
    "**Step 3.1.1: Chunk overlap** will blend together our chunks so that the tail of Chunk #1 will be the same thing and the head of Chunk #2 and so on and so forth.\n",
    "\n",
    "This time I'll load up my overlap with a value of 4, this means 4 characters of overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Retrieval-Augmented Gener', metadata={}),\n",
       " Document(page_content='eneration (RAG) is the pr', metadata={}),\n",
       " Document(page_content='e process of optimizing t', metadata={}),\n",
       " Document(page_content='ng the output of  a large', metadata={}),\n",
       " Document(page_content='arge language model, so i', metadata={}),\n",
       " Document(page_content='so it references an autho', metadata={}),\n",
       " Document(page_content='uthoritative knowledge ba', metadata={}),\n",
       " Document(page_content='e base outside  of its tr', metadata={}),\n",
       " Document(page_content='s training data sources b', metadata={}),\n",
       " Document(page_content='es before generating a re', metadata={}),\n",
       " Document(page_content='a response.', metadata={})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=25, chunk_overlap=4, separator='', strip_whitespace=True)\n",
    "document_chunks = text_splitter.create_documents([text])\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have the same chunks, but now there is overlap between 1 & 2 and 2 & 3 and so on. The 'ener' on the tail of Chunk #1 matches the 'ener' of the head of Chunk #2.\n",
    "\n",
    "To better visualize this use [ChunkViz.com](www.chunkviz.com). Here's what the same text looks like.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../98_Images/ChunkViz.png\" alt=\"image\" style=\"max-width: 800px;\">\n",
    "</div>\n",
    "\n",
    "static/ChunkVizCharacterRecursive.png\n",
    "\n",
    "Check out how we have three colors, with two overlaping sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.1.2: Separators** are character(s) sequences you would like to split on. Say if you wanted to chunk your data at `ng`, you can specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 63, which is longer than the specified 25\n",
      "Created a chunk of size 26, which is longer than the specified 25\n",
      "Created a chunk of size 83, which is longer than the specified 25\n",
      "Created a chunk of size 29, which is longer than the specified 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Retrieval-Augmented Generation (RAG) is the process of optimizi', metadata={}),\n",
       " Document(page_content='the output of  a large la', metadata={}),\n",
       " Document(page_content='uage model, so it references an authoritative knowledge base outside  of its traini', metadata={}),\n",
       " Document(page_content='data sources before generati', metadata={}),\n",
       " Document(page_content='a response.', metadata={})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(separator='ng', chunk_size=25, chunk_overlap=0)\n",
    "document_chunks = text_splitter.create_documents([text])\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if `separator` parameter is passed with a character other than `''`, it is not respecting the `chunk_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer below URL for the additional keywords that can be used while creating objects from `CharacterTextSplitter` class.  \n",
    "https://api.python.langchain.com/en/latest/base/langchain_text_splitters.base.TextSplitter.html#langchain_text_splitters.base.TextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************\n",
    "*************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4: Implement character splitting using Llama Index `SentenceSplitter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Llama Index](https://www.llamaindex.ai/) is a great choice for flexibility in the chunking and indexing process. They provide node relationships out of the box which can aid in retrieval later.\n",
    "\n",
    "Let's take a look at their sentence splitter. It is similar to the character splitter, but using its default settings, it'll split on sentences instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SentenceSplitter object\n",
    "splitter = SentenceSplitter(chunk_size=25, chunk_overlap=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"../99_Datasets/simple_rag.txt\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (11) is close to chunk size (25). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    }
   ],
   "source": [
    "# Create your nodes. Nodes are similar to documents but with more relationship data added to them.\n",
    "doc_nodes = splitter.get_nodes_from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "Retrieval-Augmented Generation (RAG) is the process of\n",
      "process of optimizing the output of  a large language model, so it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextNode(id_='5d853c82-b7cb-4325-b0b0-8c18f569c1b6', embedding=None, metadata={'file_path': '../99_Datasets/simple_rag.txt', 'file_name': 'simple_rag.txt', 'file_type': 'text/plain', 'file_size': 222, 'creation_date': '2024-04-27', 'last_modified_date': '2024-04-27'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='21392551-1b70-4e21-b41a-5009bf2cd6de', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '../99_Datasets/simple_rag.txt', 'file_name': 'simple_rag.txt', 'file_type': 'text/plain', 'file_size': 222, 'creation_date': '2024-04-27', 'last_modified_date': '2024-04-27'}, hash='788154316878a11c228a2835e9e9140c590c2110984864aa51d752ffdd55a688'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='fb63cfb5-6baf-44b1-985b-1d7da35b48bd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='aef575bcf11c9cffd8ea74825430a4bc0b173105137b6cdab11e304d699f788d')}, text='Retrieval-Augmented Generation (RAG) is the process of', start_char_idx=0, end_char_idx=54, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(doc_nodes[0].get_content()))\n",
    "print(doc_nodes[0].get_content())\n",
    "print(doc_nodes[1].get_content())\n",
    "doc_nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is a lot more relationship data held within Llama Index's nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2) Level 2: Recursive Character Text Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"RecursiveCharacterSplitting\"></a>\n",
    "Let's jump a level of complexity.\n",
    "\n",
    "The problem with Level #1 is that we don't take into account the structure of our document at all. We simply split by a fix number of characters.\n",
    "\n",
    "The Recursive Character Text Splitter helps with this. With it, we'll specify a series of separatators which will be used to split our docs.\n",
    "\n",
    "You can see the default separators for LangChain [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L842). Let's take a look at them one by one.\n",
    "\n",
    "* \"\\n\\n\" - Double new line, or most commonly paragraph breaks\n",
    "* \"\\n\" - New lines\n",
    "* \" \" - Spaces\n",
    "* \"\" - Characters\n",
    "\n",
    "I'm not sure why a period (\".\") isn't included on the list, perhaps it is not universal enough? If you know, let me know.\n",
    "\n",
    "This is the swiss army knife of splitters and my first choice when mocking up a quick application. If you don't know which splitter to start with, this is a good first bet.\n",
    "\n",
    "Let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's load up a larger piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_sentence_length: 155\n",
      "second_sentence_length: 313\n",
      "third_sentence_length: 433\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"\"\"One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\"\"\"\n",
    "first_sentence_length = len(first_sentence)\n",
    "print(f\"first_sentence_length: {first_sentence_length}\")\n",
    "\n",
    "second_sentence = \"\"\"Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\"\"\"\n",
    "second_sentence_length = len(second_sentence)\n",
    "print(f\"second_sentence_length: {second_sentence_length}\")\n",
    "\n",
    "third_sentence = \"\"\"It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\"\"\"\n",
    "third_sentence_length = len(third_sentence)\n",
    "print(f\"third_sentence_length: {third_sentence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne of the most important things I didn\\'t understand about the world when I was a child is the degree to which the returns for performance are superlinear.\\n\\nTeachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers. You get no customers, and you go out of business.\\n\\nIt\\'s obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we\\'ve invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In first iteration, the method `create_documents` will split data based on the delimiter `\\n\\n` (double new lines).  \n",
    "\n",
    "**Note:** Check the output of `text` variable above, End of each line we will have a new line character and between each sentence we another newline character. Total 2 new line characters.  \n",
    "\n",
    "**The output after 1st iteration is:**  \n",
    "\n",
    "chunk_1 = \"One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\"  \n",
    "\n",
    "chunk_2 = \"Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\"  \n",
    "\n",
    "chunk_3 = \"It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it verifies the `chunk_size`, since each sentence length is more than 65 characters, it looks for the delimiter `\\n` (new line).  \n",
    "\n",
    "**The output after 2nd iteration is:**  \n",
    "\n",
    "Since new line is not present in any of the chunk, the outpiut **remains same** after this iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most important things I didn't understand about the wo\n",
      "68\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "print(first_sentence[:65])\n",
    "print(len(\"One of the most important things I didn't understand about the world\"))\n",
    "print(len(\"world when I was a child is the degree to which the returns for performance\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it verifies the `chunk_size` again, since each sentence length is more than 65 characters, it looks for the delimiter `\" \"` (spaces).  \n",
    "\n",
    "**The output after 3rd iteration is:**\n",
    "  \n",
    "chunk_1 = \"One of the most important things I didn't understand about the\"  \n",
    "chunk_2 = \"world when I was a child is the degree to which the returns for\"  \n",
    "chunk_3 = \"performance are superlinear.\"  \n",
    ".  \n",
    ".  \n",
    "chunk_15 = \"even benefit to humanity. In all of these, the rich get richer.\"  \n",
    "chunk_16 = \"[1]\"  \n",
    "\n",
    "**Note:** First chunk, it splits at 62nd character since if it consider the next available space, the chunk size will go beyond 65 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make our text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"One of the most important things I didn't understand about the\"),\n",
       " Document(page_content='world when I was a child is the degree to which the returns for'),\n",
       " Document(page_content='performance are superlinear.'),\n",
       " Document(page_content='Teachers and coaches implicitly told us the returns were linear.'),\n",
       " Document(page_content='\"You get out,\" I heard a thousand times, \"what you put in.\" They'),\n",
       " Document(page_content='meant well, but this is rarely true. If your product is only'),\n",
       " Document(page_content=\"half as good as your competitor's, you don't get half as many\"),\n",
       " Document(page_content='customers. You get no customers, and you go out of business.'),\n",
       " Document(page_content=\"It's obviously true that the returns for performance are\"),\n",
       " Document(page_content='superlinear in business. Some think this is a flaw of'),\n",
       " Document(page_content='capitalism, and that if we changed the rules it would stop being'),\n",
       " Document(page_content='true. But superlinear returns for performance are a feature of'),\n",
       " Document(page_content=\"the world, not an artifact of rules we've invented. We see the\"),\n",
       " Document(page_content='same pattern in fame, power, military victories, knowledge, and'),\n",
       " Document(page_content='even benefit to humanity. In all of these, the rich get richer.'),\n",
       " Document(page_content='[1]')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how now there are more chunks that end with a period \".\". This is because those likely are the end of a paragraph and the splitter first looks for double new lines (paragraph break).\n",
    "\n",
    "Once paragraphs are split, then it looks at the chunk size, if a chunk is too big, then it'll split by the next separator. If the chunk is still too big, then it'll move onto the next one and so forth.\n",
    "\n",
    "For text of this size, let's split on something bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\"),\n",
       " Document(page_content='Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers. You get no customers, and you go out of business.'),\n",
       " Document(page_content=\"It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\")]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 450, chunk_overlap=0)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this text, 450 splits the paragraphs perfectly. You can even switch the chunk size to 469 and get the same splits. This is because this splitter builds in a bit of cushion and wiggle room to allow your chunks to 'snap' to the nearest separator.\n",
    "\n",
    "Let's view this visually\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/ChunkVizCharacterRecursive.png\" alt=\"image\" style=\"max-width: 800px;\">\n",
    "</div>\n",
    "\n",
    "Wow - you already made it to level 2, awesome! We're on a roll. If you like the content, I send updates to email subscribers on projects I'm working on. If you want to get the scoop, sign up [here](https://mail.gregkamradt.com/signup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3) Level 3: Document Specific Splitting <a id=\"DocumentSpecific\"></a>\n",
    "\n",
    "Stepping up our levels ladder, let's start to handle document types other than normal prose in a .txt. What if you have pictures? or a PDF? or code snippets?\n",
    "\n",
    "Our first two levels wouldn't work great for this so we'll need to find a different tactic.\n",
    "\n",
    "This level is all about making your chunking strategy fit your different data formats. Let's run through a bunch of examples of this in action\n",
    "\n",
    "The Markdown, Python, and JS splitters will basically be similar to Recursive Character, but with different separators.\n",
    "\n",
    "See all of LangChains document splitters [here](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter) and Llama Index ([HTML](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#htmlnodeparser), [JSON](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#jsonnodeparser), [Markdown](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#markdownnodeparser))\n",
    "\n",
    "#### Markdown\n",
    "\n",
    "You can see the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1175).\n",
    "\n",
    "Separators:\n",
    "* `\\n#{1,6}` - Split by new lines followed by a header (H1 through H6)\n",
    "* ```` ```\\n ```` - Code blocks\n",
    "* `\\n\\\\*\\\\*\\\\*+\\n` - Horizontal Lines\n",
    "* `\\n---+\\n` - Horizontal Lines\n",
    "* `\\n___+\\n` - Horizontal Lines\n",
    "* `\\n\\n` Double new lines\n",
    "* `\\n` - New line\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "# create an object of MarkdownTextSplitter class\n",
    "md_splitter = MarkdownTextSplitter(chunk_size=128, chunk_overlap=0)\n",
    "\n",
    "# read input markdown file and split it\n",
    "file_path = Path.cwd().parent.joinpath(\"99_Datasets\").joinpath(\"nlp_small_feedforward.md\")\n",
    "if not file_path.exists():\n",
    "    print(\"Input file not available\")\n",
    "else:\n",
    "    with open(file=file_path, mode=\"rt\") as input_file:\n",
    "        text = input_file.read()\n",
    "        doc_chunks = md_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# [NLP with Small Feed-Forward Networks](https://arxiv.org/pdf/1708.00214.pdf)', metadata={}),\n",
       " Document(page_content='by: **Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, \\nAlex Salcianu, David Weiss, Ryan McDonald, Slav Petrov (Google)**', metadata={}),\n",
       " Document(page_content='## tl;dr\\nSmall and shallow feedforward networks are memory and speed efficient, and perform surprisingly well.', metadata={}),\n",
       " Document(page_content='Using some techniques, they get close to state-of-the-art on structured and unstructured language tasks.', metadata={}),\n",
       " Document(page_content='## Notes \\n\\n#### Language tasks on a budget\\n\\nrelated work :', metadata={}),\n",
       " Document(page_content='* 900000 params only for LSTM-based POS tagging model (Gillick et al 2016)', metadata={}),\n",
       " Document(page_content='* 8.8 words / s for two-layered LSTM on Android phone for translation (Kim and Rush 2016)', metadata={}),\n",
       " Document(page_content='***\\n\\n4 tasks adressed here :', metadata={}),\n",
       " Document(page_content='* language identification\\n* POS tagging\\n* word segmentation (e.g. for chinese where word delimitation is not explicit)', metadata={}),\n",
       " Document(page_content='preordering for translation', metadata={}),\n",
       " Document(page_content='***\\n\\n4 techniques used to get better performance on budgeted models :', metadata={}),\n",
       " Document(page_content='* quantization (more dimensions, less precision)\\n* word clusters (reduce network size, use word clusters and derived features)', metadata={}),\n",
       " Document(page_content='* select features (add feature conjunctions)\\n* pipelines (allocating params for additional task in pipeline)', metadata={}),\n",
       " Document(page_content='produces models with less than 3Mb in memory\\n\\n#### Small Feed-Forward Network Models\\n\\n**model architecture**', metadata={}),\n",
       " Document(page_content='types of discrete features (eg. bigrams, trigrams...) define groups\\n\\neach group has an embedding matrix Eg', metadata={}),\n",
       " Document(page_content='embeddings extracted from groups are concatenated in one vector, which is referred to as the embedding layer output (vectors', metadata={}),\n",
       " Document(page_content='for bigrams of the same word are summed, while those from other words are concatenated)', metadata={}),\n",
       " Document(page_content='1 hidden layer with M hidden units\\n\\none softmax layer\\n\\n<img src=\"../imgs/nlpwsffn.png\" alt=\"\" style=\"width: 400px;\"/>', metadata={}),\n",
       " Document(page_content='memory use is controlled by embeddding matrix sizes\\n\\nruntime use is controlled by hidden layer size\\n\\n**hashed feature ngrams**', metadata={}),\n",
       " Document(page_content='Character ngrams are used (instead of word embeddings, because require large dictionaries and big dimensions), trained from', metadata={}),\n",
       " Document(page_content='scratch', metadata={}),\n",
       " Document(page_content='The size of the vocabulary Vg is fixed for each group and random feature mixing is used to calculate the embedding for a given', metadata={}),\n",
       " Document(page_content='ngram', metadata={}),\n",
       " Document(page_content='* means that a ngram is indexed by v = H(x) mod Vg (H being a hash function)', metadata={}),\n",
       " Document(page_content='* allows for small vocab sizes (500-1000) and thus smaller feature embeddings (16 coords)', metadata={}),\n",
       " Document(page_content='**quantization**', metadata={}),\n",
       " Document(page_content='embedding weights are compressed (from 32bits to 8bits) using a non-inversible transformation (linear + ceil) and a scaling', metadata={}),\n",
       " Document(page_content='factor that has to be stored', metadata={}),\n",
       " Document(page_content='weights are dequantized on-the-fly at runtime\\n\\n**training**\\n\\nobj. function is crossentropy + L2 reg', metadata={}),\n",
       " Document(page_content='exponentially decaying learning rates\\n\\nhyperoptimization for other hyperparams, with early stopping\\n\\n#### Experiments', metadata={}),\n",
       " Document(page_content='* averaging instead of summing embeddings inside same group and same word', metadata={}),\n",
       " Document(page_content='* using Bloom maps to represent word clusters (maps with approximate key-value retrieval)', metadata={}),\n",
       " Document(page_content='* casting unseen characters to special symbol for embedding', metadata={}),\n",
       " Document(page_content='#### Adding code block for testing MarkdownTextSplitter', metadata={}),\n",
       " Document(page_content='```\\nx = 5;\\ny = 6;\\nz = x + y;\\nprint(z)\\n```', metadata={})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the splits gravitate towards markdown sections. However, it's still not perfect.  \n",
    "Check out how there is a chunk with just **\"scratch\"**, **\"ngram\"** in it. You'll run into this at low-sized chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python\n",
    "\n",
    "See the python splitters [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1069)\n",
    "\n",
    "* `\\nclass` - Classes first\n",
    "* `\\ndef` - Functions next\n",
    "* `\\n\\tdef` - Indented functions\n",
    "* `\\n\\n` - Double New lines\n",
    "* `\\n` - New Lines\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Characters\n",
    "\n",
    "\n",
    "Let's load up our splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "py_splitter = PythonCodeTextSplitter(chunk_size=98, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_text = \"\"\"\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print (i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='class Person:\\n  def __init__(self, name, age):\\n    self.name = name\\n    self.age = age', metadata={}),\n",
       " Document(page_content='p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print (i)', metadata={})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks = py_splitter.create_documents([python_text])\n",
    "doc_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out how the class stays together in a single document (good), then the rest of the code is in a second document (ok).\n",
    "\n",
    "I needed to play with the chunk size to get a clean result like that. You'll likely need to do the same for yours which is why using evaluations to determine optimal chunk sizes is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with text in the language model world, we don't deal with raw strings.  \n",
    "It is more common to work with documents. So will consider reading `paul_graham_essays.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer the source notebook from Greg  \n",
    "https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
